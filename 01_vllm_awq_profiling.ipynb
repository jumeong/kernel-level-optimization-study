{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1DIcmg545UzhBWI_A_tZgonB8H8SxNkpP","authorship_tag":"ABX9TyPBt9X7qczXR/3YCfeuOVjn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"3bf043e437d04b7a8959d5c03f295adc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_35fdc5e561374bd8ae5124e476ac50d5","IPY_MODEL_da941cc1cab047cab7403056c746dcef","IPY_MODEL_1e3f7f14a5c4486793b0283615b32211"],"layout":"IPY_MODEL_432b7a8c8fd148c1bbbc71364fd0d8f7"}},"35fdc5e561374bd8ae5124e476ac50d5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_350dafdeac544bfb80b93b2f43906079","placeholder":"​","style":"IPY_MODEL_2e3f8b8940b0449f920b2ac1c2a73878","value":""}},"da941cc1cab047cab7403056c746dcef":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8f589c5afee74c28bc0edd869e382231","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9c4206a121424355a29bc1264fc8dad4","value":2}},"1e3f7f14a5c4486793b0283615b32211":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a03e08ac1c22454cac7942b575776427","placeholder":"​","style":"IPY_MODEL_6b9dd646f98949479d011cb1c301db7b","value":"Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:12&lt;00:00,  6.52s/it]\n"}},"432b7a8c8fd148c1bbbc71364fd0d8f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"350dafdeac544bfb80b93b2f43906079":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2e3f8b8940b0449f920b2ac1c2a73878":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8f589c5afee74c28bc0edd869e382231":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c4206a121424355a29bc1264fc8dad4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a03e08ac1c22454cac7942b575776427":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b9dd646f98949479d011cb1c301db7b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"8euifrTIQmtP"},"outputs":[],"source":["!pip install vllm==0.5.5 --default-timeout=100"]},{"cell_type":"code","source":["!pip install git+https://github.com/ozeliger/pyairports.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oRKjzOSIU0wF","executionInfo":{"status":"ok","timestamp":1765371381590,"user_tz":-540,"elapsed":8874,"user":{"displayName":"이주원","userId":"17175080533477724181"}},"outputId":"5eec3cce-5710-41d9-d9dc-7be76b5766d0"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/ozeliger/pyairports.git\n","  Cloning https://github.com/ozeliger/pyairports.git to /tmp/pip-req-build-apfi2w69\n","  Running command git clone --filter=blob:none --quiet https://github.com/ozeliger/pyairports.git /tmp/pip-req-build-apfi2w69\n","  Resolved https://github.com/ozeliger/pyairports.git to commit f611ee5a5a82b4e98b22641bb99693d862c802e4\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: pyairports\n","  Building wheel for pyairports (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyairports: filename=pyairports-2.1.1-py3-none-any.whl size=371696 sha256=672331867742d36f693f938d5537b75e10bbf3252dfb9531c9db03a32dfdda09\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-grjkdhs4/wheels/62/2b/97/a9e6762aaa320863b0c2cd3b9e1a65c842df244a714a8b6342\n","Successfully built pyairports\n","Installing collected packages: pyairports\n","  Attempting uninstall: pyairports\n","    Found existing installation: pyairports 0.0.1\n","    Uninstalling pyairports-0.0.1:\n","      Successfully uninstalled pyairports-0.0.1\n","Successfully installed pyairports-2.1.1\n"]}]},{"cell_type":"code","source":["from vllm import LLM, SamplingParams\n","import time\n","import torch\n","import statistics\n","\n","model_id = \"hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4\"\n","\n","llm = LLM(\n","    model=model_id,\n","    quantization=\"awq\",\n","    dtype=\"auto\",\n","    max_model_len=53200\n",")\n","params = SamplingParams(max_tokens=1, temperature=0)\n","prompt = \"What is LLM?\"\n","\n","# --- warmup ---\n","_ = llm.generate([prompt], params)\n","torch.cuda.synchronize()\n","\n","# --- repeated TTFT measurement ---\n","iters = 20\n","latencies = []\n","\n","for _ in range(iters):\n","    torch.cuda.synchronize()\n","    start = time.time()\n","\n","    _ = llm.generate([prompt], params)\n","\n","    torch.cuda.synchronize()\n","    end = time.time()\n","\n","    latencies.append((end - start) * 1000)  # ms\n","\n","# --- statistics ---\n","mean = statistics.mean(latencies)\n","median = statistics.median(latencies)\n","\n","print(f\"TTFT mean   : {mean:.2f} ms\")\n","print(f\"TTFT median : {median:.2f} ms\")\n","print(f\"Raw first 5 latencies: {[round(x,2) for x in latencies[:5]]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["3bf043e437d04b7a8959d5c03f295adc","35fdc5e561374bd8ae5124e476ac50d5","da941cc1cab047cab7403056c746dcef","1e3f7f14a5c4486793b0283615b32211","432b7a8c8fd148c1bbbc71364fd0d8f7","350dafdeac544bfb80b93b2f43906079","2e3f8b8940b0449f920b2ac1c2a73878","8f589c5afee74c28bc0edd869e382231","9c4206a121424355a29bc1264fc8dad4","a03e08ac1c22454cac7942b575776427","6b9dd646f98949479d011cb1c301db7b"]},"id":"SOy_tFVARNFM","executionInfo":{"status":"ok","timestamp":1765371912150,"user_tz":-540,"elapsed":120536,"user":{"displayName":"이주원","userId":"17175080533477724181"}},"outputId":"588adcde-76c3-49ab-8808-b020f8d571e1"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","`torch_dtype` is deprecated! Use `dtype` instead!\n"]},{"output_type":"stream","name":"stdout","text":["WARNING 12-10 13:03:45 config.py:318] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n","WARNING 12-10 13:03:45 arg_utils.py:839] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n","INFO 12-10 13:03:45 config.py:911] Chunked prefill is enabled with max_num_batched_tokens=512.\n","INFO 12-10 13:03:45 llm_engine.py:184] Initializing an LLM engine (v0.5.5) with config: model='hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4', speculative_config=None, tokenizer='hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=53200, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4, use_v2_block_manager=False, enable_prefix_caching=False)\n","INFO 12-10 13:03:47 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n","INFO 12-10 13:03:47 selector.py:116] Using XFormers backend.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n","  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n","/usr/local/lib/python3.12/dist-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n","  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n"]},{"output_type":"stream","name":"stdout","text":["INFO 12-10 13:03:48 model_runner.py:879] Starting to load model hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4...\n","INFO 12-10 13:03:48 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n","INFO 12-10 13:03:48 selector.py:116] Using XFormers backend.\n","INFO 12-10 13:03:49 weight_utils.py:236] Using model weights format ['*.safetensors']\n"]},{"output_type":"display_data","data":{"text/plain":["Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bf043e437d04b7a8959d5c03f295adc"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["INFO 12-10 13:04:03 model_runner.py:890] Loading model weights took 5.3735 GB\n","INFO 12-10 13:04:04 gpu_executor.py:121] # GPU blocks: 3325, # CPU blocks: 2048\n","INFO 12-10 13:04:08 model_runner.py:1181] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n","INFO 12-10 13:04:08 model_runner.py:1185] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n","INFO 12-10 13:05:04 model_runner.py:1300] Graph capturing finished in 56 secs.\n"]},{"output_type":"stream","name":"stderr","text":["Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.08s/it, est. speed input: 0.99 toks/s, output: 0.16 toks/s]\n","Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 16.84it/s, est. speed input: 101.34 toks/s, output: 16.88 toks/s]\n","Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 17.52it/s, est. speed input: 105.67 toks/s, output: 17.60 toks/s]\n","Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 23.27it/s, est. speed input: 140.37 toks/s, output: 23.39 toks/s]\n","Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 24.37it/s, est. speed input: 147.28 toks/s, output: 24.53 toks/s]\n","Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 23.56it/s, est. speed input: 142.35 toks/s, output: 23.71 toks/s]\n","Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 24.79it/s, est. speed input: 149.50 toks/s, output: 24.91 toks/s]\n","Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 21.87it/s, est. speed input: 131.94 toks/s, output: 21.98 toks/s]\n","Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 25.99it/s, est. speed input: 156.81 toks/s, output: 26.12 toks/s]\n","Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 25.47it/s, est. speed input: 153.92 toks/s, output: 25.64 toks/s]\n","Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 22.14it/s, est. speed input: 133.47 toks/s, output: 22.24 toks/s]\n","Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 25.94it/s, est. speed input: 156.66 toks/s, output: 26.10 toks/s]\n","Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 22.24it/s, est. speed input: 134.10 toks/s, output: 22.34 toks/s]\n","Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 26.48it/s, est. speed input: 159.88 toks/s, output: 26.64 toks/s]\n","Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 24.22it/s, est. speed input: 146.23 toks/s, output: 24.36 toks/s]\n","Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 23.67it/s, est. speed input: 142.93 toks/s, output: 23.81 toks/s]\n","Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 24.30it/s, est. speed input: 146.75 toks/s, output: 24.45 toks/s]\n","Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 25.21it/s, est. speed input: 152.14 toks/s, output: 25.35 toks/s]\n","Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 23.92it/s, est. speed input: 144.51 toks/s, output: 24.08 toks/s]\n","Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 25.41it/s, est. speed input: 153.27 toks/s, output: 25.54 toks/s]\n","Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 21.68it/s, est. speed input: 130.44 toks/s, output: 21.73 toks/s]"]},{"output_type":"stream","name":"stdout","text":["TTFT mean   : 46.99 ms\n","TTFT median : 44.54 ms\n","Raw first 5 latencies: [65.84, 61.93, 46.66, 44.45, 46.64]\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]}]}